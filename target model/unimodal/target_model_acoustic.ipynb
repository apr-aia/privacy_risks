{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, GRU, Bidirectional, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "import statistics\n",
        "import random\n",
        "random.seed(999)\n",
        "np.random.seed(999)\n",
        "tf.random.set_seed(999)"
      ],
      "metadata": {
        "id": "c5W6zxNGf4Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract audio and lexcial features as explained in the multimodal target model\n",
        "x_audio_data = np.array([i[0] for i in df_list])  # Audio features\n",
        "#x_text_data = np.array([i[1] for i in df_list])   # Text features\n",
        "y_data = np.array([i[2] for i in df_list])        # Depression labels (binary: 0 for control, 1 for depressed)\n",
        "unique_clip_ID_No = np.array([i[3] for i in df_list])  # Unique clip identifier\n",
        "session_number = np.array([i[4] for i in df_list])     # Session number"
      ],
      "metadata": {
        "id": "IZfgHZP9auQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target model - acoustic"
      ],
      "metadata": {
        "id": "OnlkVoe7a7IK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "NUM_FOLDS = 5\n",
        "ACOUSTIC_INPUT_SHAPE = (499, 512)\n",
        "N_CONV_LAYERS = 2\n",
        "N_CONV_KERNELS = 64\n",
        "CONV_KERNEL_WIDTH = 3\n",
        "MAX_POOLING_KERNEL_WIDTH = 2\n",
        "N_GRU_LAYERS = 2\n",
        "GRU_LAYER_WIDTH = 64\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 200\n",
        "\n",
        "# Data preparation\n",
        "speaker_ids = np.unique(session_number)\n",
        "k_fold = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
        "training_index, testing_index = [], []\n",
        "accuracies_best, loss_fold_best, F1_best = [], [], []\n",
        "session_list=[]\n",
        "\n",
        "# Define a custom early stopping callback to restore the best weights\n",
        "class CustomEarlyStopping(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, patience=10, restore_best_weights=True, path=''):\n",
        "        super().__init__()\n",
        "        self.patience = patience\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_weights = None\n",
        "        self.path = path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_loss = logs.get('val_loss')\n",
        "        if current_loss < self.best:\n",
        "            self.best = current_loss\n",
        "            self.best_weights = self.model.get_weights()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.model.stop_training = True\n",
        "                if self.restore_best_weights:\n",
        "                    self.model.set_weights(self.best_weights)\n",
        "                    self.model.save(self.path)\n",
        "                    print(f'Restoring model weights from the end of the best epoch to {self.path}')\n",
        "\n",
        "# Split data for k-fold cross-validation\n",
        "for train_idx, test_idx in k_fold.split(speaker_ids):\n",
        "    training_index.append(np.where(np.isin(session_number, speaker_ids[train_idx])))\n",
        "    testing_index.append(np.where(np.isin(session_number, speaker_ids[test_idx])))\n",
        "\n",
        "# Model training and evaluation\n",
        "for i, (train_idx, test_idx) in enumerate(zip(training_index, testing_index)):\n",
        "    print(f'Processing Fold {i + 1}')\n",
        "    x_train, y_train = x_audio_data[train_idx[0]], y_data[train_idx[0]]\n",
        "    x_test, y_test = x_audio_data[test_idx[0]], y_data[test_idx[0]]\n",
        "    session_no = np.unique(session_number[test_idx[0]])\n",
        "    session_list.append(session_no)\n",
        "\n",
        "    # Build and compile the model\n",
        "    acoustic_input = Input(shape=ACOUSTIC_INPUT_SHAPE, name=\"acoustic_input\")\n",
        "    x = acoustic_input\n",
        "    for _ in range(N_CONV_LAYERS):\n",
        "        x = Conv1D(filters=N_CONV_KERNELS, kernel_size=CONV_KERNEL_WIDTH, activation='relu')(x)\n",
        "        x = MaxPooling1D(pool_size=MAX_POOLING_KERNEL_WIDTH)(x)\n",
        "    for _ in range(N_GRU_LAYERS):\n",
        "        x = Bidirectional(GRU(GRU_LAYER_WIDTH, return_sequences=True))(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    output = Dense(2, activation='softmax')(Dense(1024, activation='relu')(x))\n",
        "    model = Model(inputs=acoustic_input, outputs=output)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Fit model with early stopping\n",
        "    checkpoint_path = f'model_checkpoint_fold_{i + 1}.h5'\n",
        "    early_stopping = CustomEarlyStopping(patience=5, restore_best_weights=True, path=checkpoint_path)\n",
        "    model.fit(x_train, y_train, validation_split=0.2, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test, y_test)\n",
        "    accuracies_best.append(accuracy)\n",
        "    loss_fold_best.append(loss)\n",
        "\n",
        "    # Predictions and F1 score\n",
        "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "    F1_score = f1_score(y_test, y_pred)\n",
        "    F1_best.append(F1_score)\n",
        "    print(f\"Fold {i + 1}: Test Loss: {loss}, Test Accuracy: {accuracy}, F1 Score: {F1_score}\")\n",
        "\n",
        "# Summary statistics across all folds\n",
        "print('--- Summary of Results ---')\n",
        "for test_set, loss, acc, f1 in zip(session_list, loss_fold_best, accuracies_best, F1_best):\n",
        "    print(f'Test set: {test_set}, Loss: {loss:.4f}, Accuracy: {acc:.4f}, F1 Score: {f1:.4f}')\n",
        "\n",
        "# Calculate and print the average metrics across all folds to provide an overall assessment\n",
        "print(' ------------------------- ')\n",
        "print(f'Average accuracy per 10 sec chunk : {np.mean(accuracies_best):.4f}, std deviation: {statistics.stdev(accuracies_best):.4f}')\n",
        "print(f'Average F1 per 10 sec chunk : {np.mean(F1_best):.4f}, std deviation: {statistics.stdev(F1_best):.4f}')"
      ],
      "metadata": {
        "id": "e-PEetlOf1Q5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}