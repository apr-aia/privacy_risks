{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, GRU, Bidirectional, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import f1_score\n",
        "import statistics"
      ],
      "metadata": {
        "id": "zZAaFt1abVYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract audio and lexcial features as explained in the multimodal target model\n",
        "#x_audio_data = np.array([i[0] for i in df_list])  # Audio features\n",
        "x_text_data = np.array([i[1] for i in df_list])   # Text features\n",
        "y_data = np.array([i[2] for i in df_list])        # Depression labels (binary: 0 for control, 1 for depressed)\n",
        "unique_clip_ID_No = np.array([i[3] for i in df_list])  # Unique clip identifier\n",
        "session_number = np.array([i[4] for i in df_list])     # Session number"
      ],
      "metadata": {
        "id": "qaPXU5k-ibwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target model - lexical"
      ],
      "metadata": {
        "id": "U_neP6x2xaT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants for the model\n",
        "NUM_FOLDS = 5\n",
        "GRU_LAYER_WIDTH = 252\n",
        "N_GRU_LAYERS = 2\n",
        "INPUT_SHAPE = (333, 768)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 200\n",
        "\n",
        "# Data preparation\n",
        "speaker_ids = np.unique(session_number)\n",
        "k_fold = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
        "training_index, testing_index, session_list = [], [], []\n",
        "accuracies_best, loss_fold_best, F1_best = [], [], []\n",
        "\n",
        "# Define a custom early stopping callback to restore the best weights\n",
        "class CustomEarlyStopping(Callback):\n",
        "    def __init__(self, patience=0, verbose=1):\n",
        "        super(CustomEarlyStopping, self).__init__()\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.best_weights = None\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_loss = logs.get('val_loss')\n",
        "        if current_loss < self.best:\n",
        "            self.best = current_loss\n",
        "            self.wait = 0\n",
        "            self.best_weights = self.model.get_weights()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                self.model.set_weights(self.best_weights)\n",
        "                if self.verbose:\n",
        "                    print(f'Restoring model weights from the end of the best epoch {epoch - self.patience}.')\n",
        "\n",
        "# Split data into folds for cross-validation\n",
        "for train_idx, test_idx in k_fold.split(speaker_ids):\n",
        "    training_index.append(np.where(np.isin(session_number, speaker_ids[train_idx])))\n",
        "    testing_index.append(np.where(np.isin(session_number, speaker_ids[test_idx])))\n",
        "\n",
        "# Model training and evaluation\n",
        "for i in range(NUM_FOLDS):\n",
        "    print(f'Processing Fold {i + 1}')\n",
        "    x_train, y_train = x_text_data[training_index[i][0]], y_data[training_index[i][0]]\n",
        "    x_test, y_test = x_text_data[testing_index[i][0]], y_data[testing_index[i][0]]\n",
        "    session_no = np.unique(session_number[testing_index[i][0]])\n",
        "    session_list.append(session_no)\n",
        "\n",
        "    print(f'Test session for this fold: {session_no}')\n",
        "\n",
        "    # Build the model\n",
        "    lexical_input = Input(shape=INPUT_SHAPE, name=\"lexical_input\")\n",
        "    x = lexical_input\n",
        "    for _ in range(N_GRU_LAYERS):\n",
        "        x = Bidirectional(GRU(GRU_LAYER_WIDTH, return_sequences=True))(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    output = Dense(2, activation='softmax')(Dense(128, activation='relu')(x))\n",
        "    model = Model(inputs=lexical_input, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Fit model with early stopping\n",
        "    early_stopping = CustomEarlyStopping(patience=3)\n",
        "    model.fit(x_train, y_train, validation_split=0.2, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping])\n",
        "    loss, accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "    print(f\"Test Loss: {loss}\")\n",
        "    print(f\"Test Accuracy: {accuracy}\")\n",
        "    accuracies_best.append(accuracy)\n",
        "    loss_fold_best.append(loss)\n",
        "\n",
        "    # Predictions and F1 score evaluation\n",
        "    y_pred = np.array([np.argmax(model.predict(np.array([x]), verbose=0)[0]) for x in x_test])\n",
        "    F1_score = f1_score(y_test, y_pred)\n",
        "    F1_best.append(F1_score)\n",
        "    print(f\"F1 Score: {F1_score}\")\n",
        "\n",
        "# Summarize results\n",
        "print('--- Summary of Results ---')\n",
        "for test_set, loss, acc, f1 in zip(session_list, loss_fold_best, accuracies_best, F1_best):\n",
        "    print(f'Test set: {test_set}, Loss: {loss:.4f}, Accuracy: {acc:.4f}, F1 Score: {f1:.4f}')\n",
        "print(f'Average accuracy per 10 sec chunk: {np.mean(accuracies_best):.4f}, Std Dev: {statistics.stdev(accuracies_best):.4f}')\n",
        "print(f'Average F1 score per 10 sec chunk: {np.mean(F1_best):.4f}, Std Dev: {statistics.stdev(F1_best):.4f}')\n"
      ],
      "metadata": {
        "id": "p-I4vQTVbPm0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}