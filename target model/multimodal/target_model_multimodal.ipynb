{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h5z5D4bvigeu"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access datasets and files stored in your Google Drive account\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Lkidv5nyjEZw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import statistics\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import pickle\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, GRU, GlobalAveragePooling1D, Dense, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "import math\n",
        "from transformers import AutoTokenizer, AutoModel, Wav2Vec2FeatureExtractor, Wav2Vec2Model, Wav2Vec2Processor, Wav2Vec2Tokenizer\n",
        "import random\n",
        "K.clear_session()\n",
        "random.seed(999)\n",
        "np.random.seed(999)\n",
        "tf.random.set_seed(999)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data processing"
      ],
      "metadata": {
        "id": "672CP3E6D856"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all necessary libraries are installed\n",
        "# !pip install transformers\n",
        "# !pip install torch\n",
        "\n",
        "# Setup for model devices, assuming CUDA is available for GPU acceleration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize tokenizer and model for BERT\n",
        "bert_model_name = \"dbmdz/bert-base-italian-uncased\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = AutoModel.from_pretrained(bert_model_name).to(device)\n",
        "\n",
        "# Initialize processor and model for Wav2Vec2\n",
        "w2v_model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-italian\"\n",
        "w2v_processor = Wav2Vec2Processor.from_pretrained(w2v_model_name)\n",
        "w2vec_model = Wav2Vec2Model.from_pretrained(w2v_model_name).to(device)\n",
        "\n",
        "def extract_features(file_path, text):\n",
        "    \"\"\"\n",
        "    Extract audio and text features using pre-trained Wav2Vec2 and BERT models.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the audio file.\n",
        "        text (str): Text data for feature extraction via BERT.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing extracted audio features and text features.\n",
        "    \"\"\"\n",
        "    # Process audio input and extract features\n",
        "    audio_input = w2v_processor(file_path, return_tensors=\"pt\", padding=True, sampling_rate=16000)\n",
        "    audio_features = w2vec_model(audio_input.input_values.to(device))\n",
        "\n",
        "    # Tokenize and extract text features\n",
        "    encoded_text = bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    text_features = bert_model(encoded_text.input_ids.to(device))\n",
        "\n",
        "    return audio_features.last_hidden_state, text_features.last_hidden_state\n",
        "\n",
        "# df_labels is the main dataframe containing all necessary metadata\n",
        "df_list = \"...\"\n",
        "for index, row in df_labels.iterrows():\n",
        "    # Directory to the audio file stored in Google Drive\n",
        "    file_url_audio = f\".../{row['audio_clip_chunk']}.wav\"\n",
        "    df_audio = pd.read_csv(file_url_audio, header=None, skiprows=1)\n",
        "    text = row['transcribed_text']\n",
        "\n",
        "    # Extract audio and text features\n",
        "    audio_features, text_features = extract_features(file_url_audio, text)\n",
        "\n",
        "    # Normalize and segment audio data into fixed timesteps\n",
        "    timesteps_audio = 499\n",
        "    step_number_audio = math.floor(len(df_audio) / timesteps_audio) * timesteps_audio\n",
        "\n",
        "    # Compile extracted features and metadata into a structured list\n",
        "    df_list.append([df_audio.iloc[0:timesteps_audio].values.flatten(), text_features.cpu().numpy().flatten(), row['depression'], row['full_speaker_id'], row['session_number']])\n",
        "\n",
        "# Convert the structured list into numpy arrays suitable for machine learning model inputs\n",
        "x_audio_data = np.array([i[0] for i in df_list])  # Audio features\n",
        "x_text_data = np.array([i[1] for i in df_list])   # Text features\n",
        "y_data = np.array([i[2] for i in df_list])        # Depression labels (binary: 0 for control, 1 for depressed)\n",
        "unique_clip_ID_No = np.array([i[3] for i in df_list])  # Unique clip identifier\n",
        "session_number = np.array([i[4] for i in df_list])     # Session number\n",
        "\n"
      ],
      "metadata": {
        "id": "yrYPVzptEfK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4202xR4_KYVg"
      },
      "source": [
        "## target model - multimodal depression detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PzD2-5wnPYu"
      },
      "outputs": [],
      "source": [
        "# Majority vote function\n",
        "\n",
        "def majority_vote(y_test, y_pred, ID):\n",
        "    \"\"\"\n",
        "    Returns the majority vote label for each unique clip_id\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({'ID': ID, 'y_test': y_test, 'y_pred': y_pred})\n",
        "\n",
        "    def majority_or_default(x):\n",
        "\n",
        "    # Ensure y_test is consistent within each ID group\n",
        "    y_test_grouped = df.groupby('ID')['y_test'].nunique()\n",
        "    if any(y_test_grouped > 1):\n",
        "        raise ValueError(\"Inconsistent y_test values within a single ID group.\")\n",
        "\n",
        "    y_test_MV = df.groupby('ID')['y_test'].first().tolist()  # Just take the first as they should all be the same\n",
        "    majority_vote_labels = df.groupby('ID')['y_pred'].apply(majority_or_default).tolist()\n",
        "\n",
        "    acc_MV = accuracy_score(y_test_MV, majority_vote_labels)\n",
        "    F1 = f1_score(y_test_MV, majority_vote_labels)\n",
        "\n",
        "    print('y_test_MV' , y_test_MV)\n",
        "    print('majority_vote_labels', majority_vote_labels)\n",
        "\n",
        "    return acc_MV, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRBnUQLkRc2-"
      },
      "outputs": [],
      "source": [
        "# Early stopping\n",
        "class CustomEarlyStopping(Callback):\n",
        "    def __init__(self, patience=10, restore_path=None):\n",
        "        super(CustomEarlyStopping, self).__init__()\n",
        "        self.patience = patience\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.prev_val_accuracy = -1\n",
        "        self.restore_path = restore_path\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.prev_val_accuracy = -1\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current = logs.get('val_accuracy')\n",
        "\n",
        "        # Check if the current accuracy is the same as the previous one\n",
        "        if current == self.prev_val_accuracy:\n",
        "            self.wait += 1\n",
        "        else:\n",
        "            self.wait = 0\n",
        "\n",
        "        # Update the previous validation accuracy\n",
        "        self.prev_val_accuracy = current\n",
        "\n",
        "        if self.wait >= self.patience:\n",
        "            self.stopped_epoch = epoch\n",
        "            self.model.stop_training = True\n",
        "            restore_weights_path = f\"checkpoint_epoch_{epoch - self.patience}.h5\"\n",
        "            self.model.load_weights(restore_weights_path)\n",
        "            # Save the restored model to the specified path\n",
        "            if self.restore_path:\n",
        "                self.model.save(self.restore_path)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print(f\"\\nEarly stopped: epoch {self.stopped_epoch}. Restored model weights from epoch {self.stopped_epoch - self.patience}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypYVoKXsduhk"
      },
      "outputs": [],
      "source": [
        "# Constants and placeholders\n",
        "num_folds = 5\n",
        "speaker_ids = np.unique(session_number)  # session_number is predefined\n",
        "k_fold = KFold(n_splits=num_folds, shuffle=True)\n",
        "training_index, testing_index = [], []\n",
        "accuracies_best, F1_best, f1_MV_best_, accuracies_MV_best, session_list = [], [], [], [], []\n",
        "\n",
        "# Splitting data into folds\n",
        "for fold_indexes_training, fold_indexes_test in k_fold.split(speaker_ids):\n",
        "    fold_test = speaker_ids[fold_indexes_test]\n",
        "    result_test = np.isin(session_number, fold_test)\n",
        "    testing_index.append(np.where(result_test))\n",
        "\n",
        "    fold_train = speaker_ids[fold_indexes_training]\n",
        "    result_train = np.isin(session_number, fold_train)\n",
        "    training_index.append(np.where(result_train))\n",
        "\n",
        "# Loop through each fold\n",
        "for i in range(num_folds):\n",
        "    print(f'Processing fold number: {i + 1}')\n",
        "\n",
        "    # Load data per fold\n",
        "    x_train_audio, x_train_text = x_audio_data[training_index[i][0]], x_text_data[training_index[i][0]]\n",
        "    y_train = y_data[training_index[i][0]]\n",
        "    x_test_audio, x_test_text = x_audio_data[testing_index[i][0]], x_text_data[testing_index[i][0]]\n",
        "    y_test = y_data[testing_index[i][0]]\n",
        "    session_no = session_number[testing_index[i][0]]\n",
        "\n",
        "    # Print test session numbers\n",
        "    unique_sessions = np.unique(session_no)\n",
        "    print(f'Test sessions for this fold: {unique_sessions}')\n",
        "    session_list.append(unique_sessions)\n",
        "\n",
        "\n",
        "    # Clip id calculate the mjority vote function\n",
        "    clip_id_testing=unique_clip_ID_No[testing_index[i][0]]\n",
        "\n",
        "    # Neural network setup\n",
        "    acoustic_input = Input(shape=(499, 512), name=\"acoustic_input\")\n",
        "    lexical_input = Input(shape=(333, 768), name=\"lexical_input\")\n",
        "\n",
        "    # Acoustic processing layers\n",
        "    x_a = acoustic_input\n",
        "    for _ in range(2):  # Two convolution layers\n",
        "        x_a = Conv1D(filters=64, kernel_size=3, activation='relu')(x_a)\n",
        "        x_a = MaxPooling1D(pool_size=2)(x_a)\n",
        "    x_a = Bidirectional(GRU(64, return_sequences=True))(x_a)\n",
        "    x_a = GlobalAveragePooling1D()(x_a)\n",
        "\n",
        "    # Lexical processing layers\n",
        "    x_l = lexical_input\n",
        "    x_l = Bidirectional(GRU(64, return_sequences=True))(x_l)\n",
        "    x_l = GlobalAveragePooling1D()(x_l)\n",
        "\n",
        "    # Concatenation of features from both modalities\n",
        "    joint_representation = tf.keras.layers.concatenate([x_a, x_l])\n",
        "    joint_representation_ = Dense(256, use_bias=False, activation=None)(joint_representation)\n",
        "\n",
        "    # Dense layers and model output\n",
        "    dense_layer = Dense(1024, activation='tanh')(joint_representation_)\n",
        "    output_layer = Dense(2, activation='softmax')(dense_layer)\n",
        "    model = Model(inputs=[acoustic_input, lexical_input], outputs=output_layer)\n",
        "\n",
        "    # Model compilation\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training the model\n",
        "    early_stopping_custom = CustomEarlyStopping(patience=5)\n",
        "    model.fit([x_train_audio, x_train_text], y_train, epochs=200, batch_size=64, validation_split=0.2, callbacks=[early_stopping_custom])\n",
        "\n",
        "    # Evaluating the model\n",
        "    loss, accuracy = model.evaluate([x_test_audio, x_test_text], y_test)\n",
        "    accuracies_best.append(accuracy)\n",
        "    loss_fold_best.append(loss)\n",
        "\n",
        "    # Steps to calculate F1 score\n",
        "    y_test_pred_best = []  # Initialize the predictions list\n",
        "    threshold = 0.5  # Adjust the threshold as needed\n",
        "\n",
        "    # Split the test data\n",
        "    x_test_acoustic = np.array(x_test_audio)\n",
        "    x_test_text = np.array(x_test_text)\n",
        "\n",
        "    # Predict using the model for each pair of audio-text test samples\n",
        "    for i in range(len(x_test_acoustic)):\n",
        "      sample_acoustic = x_test_acoustic[i]\n",
        "      sample_text = x_test_text[i]\n",
        "\n",
        "      predictions = model.predict([np.array([sample_acoustic]), np.array([sample_text])], verbose=0)\n",
        "      predicted_label = np.argmax(predictions[0])\n",
        "      y_test_pred_best.append(predicted_label)\n",
        "\n",
        "    # Convert predictions list to a numpy array\n",
        "    y_test_pred_best = np.array(y_test_pred_best)\n",
        "    # Ensure the predicted labels and actual labels have the same shape\n",
        "    assert y_test_pred_best.shape == y_test.shape, f\"Shapes mismatch! y_test_pred_best: {y_test_pred_best.shape}, y_test: {y_test.shape}\"\n",
        "\n",
        "    # Calculate F1 score\n",
        "    F1_score = f1_score(y_test, y_test_pred_best)\n",
        "    F1_best.append(F1_score)\n",
        "\n",
        "    # majority vote accuracy\n",
        "    acc_MV_best, f1_MV_best = majority_vote(y_test, y_test_pred_best, clip_id_testing)\n",
        "    accuracies_MV_best.append(acc_MV_best)\n",
        "    f1_MV_best_.append(f1_MV_best)\n",
        "\n",
        "# Print average and standard deviation of accuracies and F1 scores\n",
        "print(f'Average accuracy per frame: {np.mean(accuracies_best):.4f}, std: {statistics.stdev(accuracies_best):.4f}')\n",
        "print(f'Average F1 score per frame: {np.mean(F1_best):.4f}, std: {statistics.stdev(F1_best):.4f}')\n",
        "print(f'Average accuracy per clip: {np.mean(accuracies_MV_best):.4f}, std: {statistics.stdev(accuracies_MV_best):.4f}')\n",
        "print(f'Average F1 score per clip: {np.mean(f1_MV_best_):.4f}, std: {statistics.stdev(f1_MV_best_):.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}